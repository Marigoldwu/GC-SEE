The key points of this experiment: default
random seed: 325
====================Training loop No.1====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.3179		nmi: 0.0339		ari: 0.0038		f1: 0.0965		
epoch: 002		acc: 0.4247		nmi: 0.2057		ari: 0.1078		f1: 0.1977		
epoch: 003		acc: 0.4295		nmi: 0.2358		ari: 0.0978		f1: 0.2250		
epoch: 004		acc: 0.4490		nmi: 0.2818		ari: 0.1638		f1: 0.2623		
epoch: 005		acc: 0.5133		nmi: 0.3600		ari: 0.2484		f1: 0.4723		
epoch: 006		acc: 0.5247		nmi: 0.3644		ari: 0.2515		f1: 0.4138		
epoch: 007		acc: 0.4985		nmi: 0.3851		ari: 0.2411		f1: 0.4016		
epoch: 008		acc: 0.5949		nmi: 0.4456		ari: 0.3356		f1: 0.4816		
epoch: 009		acc: 0.6407		nmi: 0.4576		ari: 0.4072		f1: 0.5144		
epoch: 010		acc: 0.6507		nmi: 0.4438		ari: 0.4171		f1: 0.5183		
epoch: 011		acc: 0.6599		nmi: 0.4500		ari: 0.3987		f1: 0.5511		
epoch: 012		acc: 0.6987		nmi: 0.4723		ari: 0.4498		f1: 0.6144		
epoch: 013		acc: 0.7408		nmi: 0.5157		ari: 0.5193		f1: 0.6976		
epoch: 014		acc: 0.7463		nmi: 0.5294		ari: 0.5184		f1: 0.7259		
epoch: 015		acc: 0.7301		nmi: 0.5180		ari: 0.4916		f1: 0.7131		
epoch: 016		acc: 0.7134		nmi: 0.5132		ari: 0.4707		f1: 0.6936		
epoch: 017		acc: 0.6883		nmi: 0.4988		ari: 0.4477		f1: 0.6575		
epoch: 018		acc: 0.6621		nmi: 0.4867		ari: 0.4226		f1: 0.6086		
epoch: 019		acc: 0.6547		nmi: 0.4869		ari: 0.4231		f1: 0.5804		
epoch: 020		acc: 0.6640		nmi: 0.4884		ari: 0.4407		f1: 0.5848		
epoch: 021		acc: 0.6806		nmi: 0.4849		ari: 0.4594		f1: 0.6163		
epoch: 022		acc: 0.7035		nmi: 0.4961		ari: 0.4779		f1: 0.6625		
epoch: 023		acc: 0.7182		nmi: 0.5056		ari: 0.4930		f1: 0.6869		
epoch: 024		acc: 0.7260		nmi: 0.5149		ari: 0.5011		f1: 0.7000		
epoch: 025		acc: 0.7293		nmi: 0.5244		ari: 0.5030		f1: 0.7064		
epoch: 026		acc: 0.7256		nmi: 0.5269		ari: 0.4974		f1: 0.7038		
epoch: 027		acc: 0.7190		nmi: 0.5212		ari: 0.4825		f1: 0.7001		
epoch: 028		acc: 0.7145		nmi: 0.5175		ari: 0.4747		f1: 0.6959		
epoch: 029		acc: 0.7149		nmi: 0.5189		ari: 0.4758		f1: 0.6964		
epoch: 030		acc: 0.7197		nmi: 0.5232		ari: 0.4858		f1: 0.6994		
epoch: 031		acc: 0.7253		nmi: 0.5272		ari: 0.4952		f1: 0.7043		
epoch: 032		acc: 0.7282		nmi: 0.5300		ari: 0.5027		f1: 0.7059		
epoch: 033		acc: 0.7293		nmi: 0.5294		ari: 0.5047		f1: 0.7070		
epoch: 034		acc: 0.7312		nmi: 0.5294		ari: 0.5074		f1: 0.7087		
epoch: 035		acc: 0.7308		nmi: 0.5279		ari: 0.5065		f1: 0.7087		
epoch: 036		acc: 0.7293		nmi: 0.5271		ari: 0.5046		f1: 0.7073		
epoch: 037		acc: 0.7290		nmi: 0.5266		ari: 0.5043		f1: 0.7068		
epoch: 038		acc: 0.7282		nmi: 0.5267		ari: 0.5033		f1: 0.7061		
epoch: 039		acc: 0.7282		nmi: 0.5272		ari: 0.5034		f1: 0.7061		
epoch: 040		acc: 0.7271		nmi: 0.5264		ari: 0.5014		f1: 0.7052		
epoch: 041		acc: 0.7253		nmi: 0.5241		ari: 0.4987		f1: 0.7031		
epoch: 042		acc: 0.7260		nmi: 0.5252		ari: 0.4999		f1: 0.7038		
epoch: 043		acc: 0.7256		nmi: 0.5247		ari: 0.4989		f1: 0.7034		
epoch: 044		acc: 0.7253		nmi: 0.5248		ari: 0.4983		f1: 0.7031		
epoch: 045		acc: 0.7253		nmi: 0.5244		ari: 0.4978		f1: 0.7032		
epoch: 046		acc: 0.7249		nmi: 0.5240		ari: 0.4970		f1: 0.7029		
epoch: 047		acc: 0.7245		nmi: 0.5237		ari: 0.4967		f1: 0.7024		
epoch: 048		acc: 0.7245		nmi: 0.5239		ari: 0.4969		f1: 0.7025		
epoch: 049		acc: 0.7242		nmi: 0.5232		ari: 0.4962		f1: 0.7021		
epoch: 050		acc: 0.7242		nmi: 0.5232		ari: 0.4962		f1: 0.7021		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1093.38 MB.
Time consuming: 16.72s or 0.28m
====================Training loop No.2====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.4457		nmi: 0.2124		ari: 0.1512		f1: 0.2475		
epoch: 002		acc: 0.3549		nmi: 0.1437		ari: 0.0808		f1: 0.1608		
epoch: 003		acc: 0.3948		nmi: 0.1855		ari: 0.0529		f1: 0.1794		
epoch: 004		acc: 0.3933		nmi: 0.1874		ari: 0.0506		f1: 0.1813		
epoch: 005		acc: 0.5336		nmi: 0.3539		ari: 0.1928		f1: 0.4165		
epoch: 006		acc: 0.5255		nmi: 0.3914		ari: 0.2704		f1: 0.4579		
epoch: 007		acc: 0.5864		nmi: 0.4403		ari: 0.2932		f1: 0.5318		
epoch: 008		acc: 0.6204		nmi: 0.4685		ari: 0.3763		f1: 0.5121		
epoch: 009		acc: 0.6451		nmi: 0.4659		ari: 0.4189		f1: 0.5275		
epoch: 010		acc: 0.6566		nmi: 0.4479		ari: 0.4225		f1: 0.5438		
epoch: 011		acc: 0.6606		nmi: 0.4421		ari: 0.4118		f1: 0.5767		
epoch: 012		acc: 0.6798		nmi: 0.4621		ari: 0.4369		f1: 0.6152		
epoch: 013		acc: 0.7105		nmi: 0.4990		ari: 0.4847		f1: 0.6684		
epoch: 014		acc: 0.7234		nmi: 0.5113		ari: 0.5059		f1: 0.6927		
epoch: 015		acc: 0.7301		nmi: 0.5185		ari: 0.5058		f1: 0.7072		
epoch: 016		acc: 0.7249		nmi: 0.5168		ari: 0.4953		f1: 0.6996		
epoch: 017		acc: 0.7042		nmi: 0.5077		ari: 0.4661		f1: 0.6685		
epoch: 018		acc: 0.6861		nmi: 0.5075		ari: 0.4501		f1: 0.6281		
epoch: 019		acc: 0.6791		nmi: 0.5042		ari: 0.4475		f1: 0.6156		
epoch: 020		acc: 0.6902		nmi: 0.5006		ari: 0.4651		f1: 0.6391		
epoch: 021		acc: 0.7001		nmi: 0.4958		ari: 0.4768		f1: 0.6596		
epoch: 022		acc: 0.7086		nmi: 0.4999		ari: 0.4868		f1: 0.6753		
epoch: 023		acc: 0.7109		nmi: 0.5076		ari: 0.4934		f1: 0.6804		
epoch: 024		acc: 0.7142		nmi: 0.5169		ari: 0.4975		f1: 0.6872		
epoch: 025		acc: 0.7201		nmi: 0.5247		ari: 0.5006		f1: 0.6958		
epoch: 026		acc: 0.7234		nmi: 0.5270		ari: 0.4974		f1: 0.7013		
epoch: 027		acc: 0.7216		nmi: 0.5226		ari: 0.4887		f1: 0.7013		
epoch: 028		acc: 0.7256		nmi: 0.5258		ari: 0.4912		f1: 0.7065		
epoch: 029		acc: 0.7297		nmi: 0.5301		ari: 0.4996		f1: 0.7097		
epoch: 030		acc: 0.7308		nmi: 0.5304		ari: 0.5038		f1: 0.7101		
epoch: 031		acc: 0.7304		nmi: 0.5308		ari: 0.5080		f1: 0.7080		
epoch: 032		acc: 0.7286		nmi: 0.5296		ari: 0.5089		f1: 0.7050		
epoch: 033		acc: 0.7242		nmi: 0.5270		ari: 0.5061		f1: 0.6991		
epoch: 034		acc: 0.7208		nmi: 0.5264		ari: 0.5035		f1: 0.6953		
epoch: 035		acc: 0.7175		nmi: 0.5234		ari: 0.4993		f1: 0.6918		
epoch: 036		acc: 0.7179		nmi: 0.5240		ari: 0.4993		f1: 0.6923		
epoch: 037		acc: 0.7182		nmi: 0.5239		ari: 0.4993		f1: 0.6926		
epoch: 038		acc: 0.7216		nmi: 0.5265		ari: 0.5026		f1: 0.6965		
epoch: 039		acc: 0.7234		nmi: 0.5289		ari: 0.5055		f1: 0.6983		
epoch: 040		acc: 0.7245		nmi: 0.5303		ari: 0.5061		f1: 0.7000		
epoch: 041		acc: 0.7253		nmi: 0.5300		ari: 0.5069		f1: 0.7006		
epoch: 042		acc: 0.7256		nmi: 0.5294		ari: 0.5069		f1: 0.7012		
epoch: 043		acc: 0.7256		nmi: 0.5287		ari: 0.5064		f1: 0.7012		
epoch: 044		acc: 0.7256		nmi: 0.5277		ari: 0.5056		f1: 0.7015		
epoch: 045		acc: 0.7256		nmi: 0.5277		ari: 0.5056		f1: 0.7015		
epoch: 046		acc: 0.7253		nmi: 0.5271		ari: 0.5050		f1: 0.7011		
epoch: 047		acc: 0.7253		nmi: 0.5279		ari: 0.5054		f1: 0.7012		
epoch: 048		acc: 0.7253		nmi: 0.5279		ari: 0.5054		f1: 0.7012		
epoch: 049		acc: 0.7253		nmi: 0.5279		ari: 0.5054		f1: 0.7012		
epoch: 050		acc: 0.7253		nmi: 0.5279		ari: 0.5054		f1: 0.7012		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1094.90 MB.
Time consuming: 12.59s or 0.21m
====================Training loop No.3====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.4357		nmi: 0.2049		ari: 0.1614		f1: 0.2469		
epoch: 002		acc: 0.3911		nmi: 0.1684		ari: 0.0866		f1: 0.2039		
epoch: 003		acc: 0.4132		nmi: 0.2144		ari: 0.0790		f1: 0.1907		
epoch: 004		acc: 0.4191		nmi: 0.2204		ari: 0.0756		f1: 0.2152		
epoch: 005		acc: 0.5332		nmi: 0.3566		ari: 0.1915		f1: 0.3592		
epoch: 006		acc: 0.5499		nmi: 0.3948		ari: 0.2888		f1: 0.4412		
epoch: 007		acc: 0.5971		nmi: 0.4682		ari: 0.3266		f1: 0.5358		
epoch: 008		acc: 0.6237		nmi: 0.4827		ari: 0.3889		f1: 0.5109		
epoch: 009		acc: 0.6507		nmi: 0.4812		ari: 0.4323		f1: 0.5243		
epoch: 010		acc: 0.6529		nmi: 0.4621		ari: 0.4275		f1: 0.5233		
epoch: 011		acc: 0.6518		nmi: 0.4506		ari: 0.4154		f1: 0.5342		
epoch: 012		acc: 0.6787		nmi: 0.4701		ari: 0.4415		f1: 0.6105		
epoch: 013		acc: 0.7038		nmi: 0.4935		ari: 0.4770		f1: 0.6569		
epoch: 014		acc: 0.7230		nmi: 0.5206		ari: 0.5071		f1: 0.6895		
epoch: 015		acc: 0.7334		nmi: 0.5335		ari: 0.5149		f1: 0.7094		
epoch: 016		acc: 0.7378		nmi: 0.5379		ari: 0.5150		f1: 0.7182		
epoch: 017		acc: 0.7260		nmi: 0.5251		ari: 0.4926		f1: 0.7067		
epoch: 018		acc: 0.7164		nmi: 0.5202		ari: 0.4774		f1: 0.6918		
epoch: 019		acc: 0.7127		nmi: 0.5212		ari: 0.4727		f1: 0.6827		
epoch: 020		acc: 0.7112		nmi: 0.5173		ari: 0.4754		f1: 0.6780		
epoch: 021		acc: 0.7212		nmi: 0.5183		ari: 0.4927		f1: 0.6917		
epoch: 022		acc: 0.7253		nmi: 0.5161		ari: 0.4997		f1: 0.7000		
epoch: 023		acc: 0.7249		nmi: 0.5179		ari: 0.5032		f1: 0.7006		
epoch: 024		acc: 0.7238		nmi: 0.5209		ari: 0.5047		f1: 0.6996		
epoch: 025		acc: 0.7293		nmi: 0.5324		ari: 0.5121		f1: 0.7078		
epoch: 026		acc: 0.7323		nmi: 0.5382		ari: 0.5132		f1: 0.7132		
epoch: 027		acc: 0.7308		nmi: 0.5371		ari: 0.5055		f1: 0.7137		
epoch: 028		acc: 0.7315		nmi: 0.5338		ari: 0.5008		f1: 0.7158		
epoch: 029		acc: 0.7345		nmi: 0.5352		ari: 0.5042		f1: 0.7183		
epoch: 030		acc: 0.7371		nmi: 0.5378		ari: 0.5105		f1: 0.7195		
epoch: 031		acc: 0.7371		nmi: 0.5380		ari: 0.5138		f1: 0.7178		
epoch: 032		acc: 0.7360		nmi: 0.5364		ari: 0.5149		f1: 0.7153		
epoch: 033		acc: 0.7341		nmi: 0.5346		ari: 0.5150		f1: 0.7120		
epoch: 034		acc: 0.7323		nmi: 0.5364		ari: 0.5158		f1: 0.7092		
epoch: 035		acc: 0.7315		nmi: 0.5366		ari: 0.5158		f1: 0.7087		
epoch: 036		acc: 0.7297		nmi: 0.5350		ari: 0.5113		f1: 0.7082		
epoch: 037		acc: 0.7290		nmi: 0.5363		ari: 0.5109		f1: 0.7077		
epoch: 038		acc: 0.7275		nmi: 0.5345		ari: 0.5076		f1: 0.7063		
epoch: 039		acc: 0.7267		nmi: 0.5344		ari: 0.5058		f1: 0.7060		
epoch: 040		acc: 0.7278		nmi: 0.5342		ari: 0.5061		f1: 0.7074		
epoch: 041		acc: 0.7278		nmi: 0.5345		ari: 0.5059		f1: 0.7075		
epoch: 042		acc: 0.7282		nmi: 0.5345		ari: 0.5066		f1: 0.7074		
epoch: 043		acc: 0.7286		nmi: 0.5348		ari: 0.5069		f1: 0.7079		
epoch: 044		acc: 0.7286		nmi: 0.5347		ari: 0.5070		f1: 0.7077		
epoch: 045		acc: 0.7286		nmi: 0.5343		ari: 0.5063		f1: 0.7078		
epoch: 046		acc: 0.7290		nmi: 0.5341		ari: 0.5067		f1: 0.7081		
epoch: 047		acc: 0.7290		nmi: 0.5341		ari: 0.5067		f1: 0.7081		
epoch: 048		acc: 0.7286		nmi: 0.5334		ari: 0.5063		f1: 0.7075		
epoch: 049		acc: 0.7286		nmi: 0.5334		ari: 0.5063		f1: 0.7075		
epoch: 050		acc: 0.7286		nmi: 0.5334		ari: 0.5063		f1: 0.7075		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1094.90 MB.
Time consuming: 12.6s or 0.21m
====================Training loop No.4====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.4531		nmi: 0.2211		ari: 0.1416		f1: 0.3074		
epoch: 002		acc: 0.4501		nmi: 0.2795		ari: 0.2133		f1: 0.2959		
epoch: 003		acc: 0.3748		nmi: 0.1420		ari: 0.0274		f1: 0.1781		
epoch: 004		acc: 0.3970		nmi: 0.1798		ari: 0.0416		f1: 0.2189		
epoch: 005		acc: 0.5340		nmi: 0.3513		ari: 0.1942		f1: 0.4214		
epoch: 006		acc: 0.5558		nmi: 0.4075		ari: 0.2598		f1: 0.5125		
epoch: 007		acc: 0.6016		nmi: 0.4557		ari: 0.2919		f1: 0.5469		
epoch: 008		acc: 0.6315		nmi: 0.4612		ari: 0.3746		f1: 0.5331		
epoch: 009		acc: 0.6355		nmi: 0.4371		ari: 0.3957		f1: 0.5165		
epoch: 010		acc: 0.6407		nmi: 0.4291		ari: 0.3981		f1: 0.5228		
epoch: 011		acc: 0.6828		nmi: 0.4610		ari: 0.4375		f1: 0.6283		
epoch: 012		acc: 0.7186		nmi: 0.5110		ari: 0.4904		f1: 0.6916		
epoch: 013		acc: 0.7278		nmi: 0.5320		ari: 0.5093		f1: 0.7072		
epoch: 014		acc: 0.7256		nmi: 0.5267		ari: 0.4969		f1: 0.7100		
epoch: 015		acc: 0.7205		nmi: 0.5196		ari: 0.4832		f1: 0.7070		
epoch: 016		acc: 0.7275		nmi: 0.5269		ari: 0.4871		f1: 0.7154		
epoch: 017		acc: 0.7194		nmi: 0.5174		ari: 0.4763		f1: 0.7027		
epoch: 018		acc: 0.7024		nmi: 0.5047		ari: 0.4612		f1: 0.6676		
epoch: 019		acc: 0.6887		nmi: 0.4912		ari: 0.4472		f1: 0.6358		
epoch: 020		acc: 0.6824		nmi: 0.4829		ari: 0.4458		f1: 0.6280		
epoch: 021		acc: 0.6928		nmi: 0.4847		ari: 0.4599		f1: 0.6520		
epoch: 022		acc: 0.7005		nmi: 0.4900		ari: 0.4687		f1: 0.6700		
epoch: 023		acc: 0.7061		nmi: 0.5002		ari: 0.4762		f1: 0.6812		
epoch: 024		acc: 0.7101		nmi: 0.5123		ari: 0.4821		f1: 0.6894		
epoch: 025		acc: 0.7164		nmi: 0.5218		ari: 0.4867		f1: 0.6989		
epoch: 026		acc: 0.7179		nmi: 0.5214		ari: 0.4834		f1: 0.7021		
epoch: 027		acc: 0.7157		nmi: 0.5161		ari: 0.4748		f1: 0.7008		
epoch: 028		acc: 0.7182		nmi: 0.5147		ari: 0.4753		f1: 0.7034		
epoch: 029		acc: 0.7238		nmi: 0.5198		ari: 0.4836		f1: 0.7076		
epoch: 030		acc: 0.7260		nmi: 0.5220		ari: 0.4895		f1: 0.7086		
epoch: 031		acc: 0.7245		nmi: 0.5210		ari: 0.4903		f1: 0.7055		
epoch: 032		acc: 0.7230		nmi: 0.5205		ari: 0.4911		f1: 0.7032		
epoch: 033		acc: 0.7227		nmi: 0.5219		ari: 0.4931		f1: 0.7024		
epoch: 034		acc: 0.7208		nmi: 0.5210		ari: 0.4920		f1: 0.6999		
epoch: 035		acc: 0.7201		nmi: 0.5215		ari: 0.4916		f1: 0.6992		
epoch: 036		acc: 0.7171		nmi: 0.5193		ari: 0.4877		f1: 0.6966		
epoch: 037		acc: 0.7171		nmi: 0.5213		ari: 0.4879		f1: 0.6969		
epoch: 038		acc: 0.7160		nmi: 0.5212		ari: 0.4862		f1: 0.6959		
epoch: 039		acc: 0.7179		nmi: 0.5227		ari: 0.4877		f1: 0.6983		
epoch: 040		acc: 0.7179		nmi: 0.5211		ari: 0.4873		f1: 0.6984		
epoch: 041		acc: 0.7197		nmi: 0.5226		ari: 0.4902		f1: 0.7000		
epoch: 042		acc: 0.7197		nmi: 0.5215		ari: 0.4913		f1: 0.6990		
epoch: 043		acc: 0.7201		nmi: 0.5210		ari: 0.4914		f1: 0.6994		
epoch: 044		acc: 0.7194		nmi: 0.5195		ari: 0.4902		f1: 0.6986		
epoch: 045		acc: 0.7197		nmi: 0.5200		ari: 0.4907		f1: 0.6990		
epoch: 046		acc: 0.7194		nmi: 0.5192		ari: 0.4897		f1: 0.6987		
epoch: 047		acc: 0.7197		nmi: 0.5194		ari: 0.4899		f1: 0.6992		
epoch: 048		acc: 0.7201		nmi: 0.5198		ari: 0.4907		f1: 0.6995		
epoch: 049		acc: 0.7208		nmi: 0.5203		ari: 0.4916		f1: 0.7002		
epoch: 050		acc: 0.7208		nmi: 0.5203		ari: 0.4916		f1: 0.7002		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1094.90 MB.
Time consuming: 12.72s or 0.21m
====================Training loop No.5====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.3859		nmi: 0.1371		ari: 0.0301		f1: 0.1987		
epoch: 002		acc: 0.4586		nmi: 0.2867		ari: 0.1865		f1: 0.3236		
epoch: 003		acc: 0.4686		nmi: 0.2665		ari: 0.1010		f1: 0.2984		
epoch: 004		acc: 0.4956		nmi: 0.3034		ari: 0.1414		f1: 0.3235		
epoch: 005		acc: 0.5126		nmi: 0.3263		ari: 0.2080		f1: 0.3643		
epoch: 006		acc: 0.5055		nmi: 0.3610		ari: 0.2133		f1: 0.4621		
epoch: 007		acc: 0.5547		nmi: 0.4050		ari: 0.2522		f1: 0.4872		
epoch: 008		acc: 0.5997		nmi: 0.4270		ari: 0.3354		f1: 0.4891		
epoch: 009		acc: 0.6318		nmi: 0.4444		ari: 0.3910		f1: 0.5095		
epoch: 010		acc: 0.6514		nmi: 0.4382		ari: 0.4105		f1: 0.5598		
epoch: 011		acc: 0.6983		nmi: 0.4847		ari: 0.4681		f1: 0.6623		
epoch: 012		acc: 0.7216		nmi: 0.5255		ari: 0.5047		f1: 0.7001		
epoch: 013		acc: 0.7138		nmi: 0.5151		ari: 0.4844		f1: 0.6991		
epoch: 014		acc: 0.7020		nmi: 0.5004		ari: 0.4581		f1: 0.6915		
epoch: 015		acc: 0.7094		nmi: 0.5032		ari: 0.4599		f1: 0.6999		
epoch: 016		acc: 0.7112		nmi: 0.5070		ari: 0.4662		f1: 0.6921		
epoch: 017		acc: 0.6913		nmi: 0.4978		ari: 0.4462		f1: 0.6437		
epoch: 018		acc: 0.6684		nmi: 0.4912		ari: 0.4262		f1: 0.5935		
epoch: 019		acc: 0.6558		nmi: 0.4837		ari: 0.4234		f1: 0.5683		
epoch: 020		acc: 0.6651		nmi: 0.4773		ari: 0.4395		f1: 0.5872		
epoch: 021		acc: 0.6758		nmi: 0.4769		ari: 0.4501		f1: 0.6239		
epoch: 022		acc: 0.6968		nmi: 0.4902		ari: 0.4683		f1: 0.6635		
epoch: 023		acc: 0.7057		nmi: 0.5018		ari: 0.4768		f1: 0.6822		
epoch: 024		acc: 0.7061		nmi: 0.5071		ari: 0.4765		f1: 0.6867		
epoch: 025		acc: 0.7168		nmi: 0.5202		ari: 0.4890		f1: 0.6991		
epoch: 026		acc: 0.7179		nmi: 0.5183		ari: 0.4852		f1: 0.7014		
epoch: 027		acc: 0.7242		nmi: 0.5221		ari: 0.4868		f1: 0.7092		
epoch: 028		acc: 0.7245		nmi: 0.5201		ari: 0.4854		f1: 0.7087		
epoch: 029		acc: 0.7264		nmi: 0.5199		ari: 0.4874		f1: 0.7103		
epoch: 030		acc: 0.7312		nmi: 0.5253		ari: 0.4961		f1: 0.7138		
epoch: 031		acc: 0.7297		nmi: 0.5237		ari: 0.4970		f1: 0.7107		
epoch: 032		acc: 0.7312		nmi: 0.5259		ari: 0.5008		f1: 0.7116		
epoch: 033		acc: 0.7297		nmi: 0.5261		ari: 0.5029		f1: 0.7085		
epoch: 034		acc: 0.7297		nmi: 0.5281		ari: 0.5053		f1: 0.7083		
epoch: 035		acc: 0.7271		nmi: 0.5276		ari: 0.5047		f1: 0.7048		
epoch: 036		acc: 0.7253		nmi: 0.5271		ari: 0.5033		f1: 0.7028		
epoch: 037		acc: 0.7223		nmi: 0.5245		ari: 0.4981		f1: 0.7001		
epoch: 038		acc: 0.7212		nmi: 0.5237		ari: 0.4964		f1: 0.6993		
epoch: 039		acc: 0.7208		nmi: 0.5226		ari: 0.4957		f1: 0.6990		
epoch: 040		acc: 0.7208		nmi: 0.5241		ari: 0.4954		f1: 0.6994		
epoch: 041		acc: 0.7219		nmi: 0.5236		ari: 0.4956		f1: 0.7008		
epoch: 042		acc: 0.7227		nmi: 0.5247		ari: 0.4965		f1: 0.7017		
epoch: 043		acc: 0.7219		nmi: 0.5227		ari: 0.4958		f1: 0.7004		
epoch: 044		acc: 0.7216		nmi: 0.5218		ari: 0.4952		f1: 0.7000		
epoch: 045		acc: 0.7223		nmi: 0.5228		ari: 0.4959		f1: 0.7008		
epoch: 046		acc: 0.7223		nmi: 0.5224		ari: 0.4956		f1: 0.7009		
epoch: 047		acc: 0.7227		nmi: 0.5228		ari: 0.4959		f1: 0.7013		
epoch: 048		acc: 0.7227		nmi: 0.5228		ari: 0.4959		f1: 0.7013		
epoch: 049		acc: 0.7227		nmi: 0.5228		ari: 0.4959		f1: 0.7013		
epoch: 050		acc: 0.7227		nmi: 0.5228		ari: 0.4959		f1: 0.7013		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1094.90 MB.
Time consuming: 12.8s or 0.21m
====================Training loop No.6====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.3600		nmi: 0.0818		ari: 0.0350		f1: 0.1709		
epoch: 002		acc: 0.4383		nmi: 0.2179		ari: 0.1024		f1: 0.2248		
epoch: 003		acc: 0.3929		nmi: 0.1811		ari: 0.0505		f1: 0.1792		
epoch: 004		acc: 0.4701		nmi: 0.2944		ari: 0.1310		f1: 0.2825		
epoch: 005		acc: 0.5236		nmi: 0.3401		ari: 0.2475		f1: 0.4549		
epoch: 006		acc: 0.4823		nmi: 0.3397		ari: 0.1883		f1: 0.4155		
epoch: 007		acc: 0.5111		nmi: 0.3662		ari: 0.2003		f1: 0.4324		
epoch: 008		acc: 0.5971		nmi: 0.4172		ari: 0.3252		f1: 0.4776		
epoch: 009		acc: 0.6425		nmi: 0.4501		ari: 0.4117		f1: 0.5098		
epoch: 010		acc: 0.6477		nmi: 0.4424		ari: 0.4162		f1: 0.5153		
epoch: 011		acc: 0.6625		nmi: 0.4425		ari: 0.4213		f1: 0.5629		
epoch: 012		acc: 0.7190		nmi: 0.4957		ari: 0.4809		f1: 0.6831		
epoch: 013		acc: 0.7441		nmi: 0.5247		ari: 0.5226		f1: 0.7193		
epoch: 014		acc: 0.7448		nmi: 0.5290		ari: 0.5186		f1: 0.7247		
epoch: 015		acc: 0.7360		nmi: 0.5216		ari: 0.4971		f1: 0.7194		
epoch: 016		acc: 0.7275		nmi: 0.5177		ari: 0.4837		f1: 0.7086		
epoch: 017		acc: 0.7094		nmi: 0.5035		ari: 0.4631		f1: 0.6791		
epoch: 018		acc: 0.7016		nmi: 0.5061		ari: 0.4565		f1: 0.6615		
epoch: 019		acc: 0.6994		nmi: 0.5125		ari: 0.4587		f1: 0.6518		
epoch: 020		acc: 0.6891		nmi: 0.5081		ari: 0.4580		f1: 0.6352		
epoch: 021		acc: 0.6857		nmi: 0.5017		ari: 0.4638		f1: 0.6336		
epoch: 022		acc: 0.7035		nmi: 0.5101		ari: 0.4847		f1: 0.6630		
epoch: 023		acc: 0.7186		nmi: 0.5169		ari: 0.5003		f1: 0.6877		
epoch: 024		acc: 0.7249		nmi: 0.5246		ari: 0.5079		f1: 0.6989		
epoch: 025		acc: 0.7264		nmi: 0.5271		ari: 0.5048		f1: 0.7039		
epoch: 026		acc: 0.7312		nmi: 0.5322		ari: 0.5055		f1: 0.7118		
epoch: 027		acc: 0.7264		nmi: 0.5283		ari: 0.4938		f1: 0.7077		
epoch: 028		acc: 0.7242		nmi: 0.5254		ari: 0.4851		f1: 0.7075		
epoch: 029		acc: 0.7256		nmi: 0.5292		ari: 0.4862		f1: 0.7093		
epoch: 030		acc: 0.7267		nmi: 0.5287		ari: 0.4884		f1: 0.7103		
epoch: 031		acc: 0.7282		nmi: 0.5267		ari: 0.4919		f1: 0.7111		
epoch: 032		acc: 0.7293		nmi: 0.5281		ari: 0.4969		f1: 0.7108		
epoch: 033		acc: 0.7293		nmi: 0.5294		ari: 0.5016		f1: 0.7092		
epoch: 034		acc: 0.7286		nmi: 0.5291		ari: 0.5035		f1: 0.7072		
epoch: 035		acc: 0.7297		nmi: 0.5309		ari: 0.5076		f1: 0.7073		
epoch: 036		acc: 0.7282		nmi: 0.5309		ari: 0.5068		f1: 0.7054		
epoch: 037		acc: 0.7278		nmi: 0.5314		ari: 0.5077		f1: 0.7048		
epoch: 038		acc: 0.7297		nmi: 0.5343		ari: 0.5103		f1: 0.7067		
epoch: 039		acc: 0.7293		nmi: 0.5332		ari: 0.5079		f1: 0.7069		
epoch: 040		acc: 0.7301		nmi: 0.5330		ari: 0.5083		f1: 0.7077		
epoch: 041		acc: 0.7297		nmi: 0.5313		ari: 0.5068		f1: 0.7071		
epoch: 042		acc: 0.7286		nmi: 0.5291		ari: 0.5041		f1: 0.7062		
epoch: 043		acc: 0.7278		nmi: 0.5282		ari: 0.5029		f1: 0.7055		
epoch: 044		acc: 0.7275		nmi: 0.5272		ari: 0.5025		f1: 0.7048		
epoch: 045		acc: 0.7278		nmi: 0.5276		ari: 0.5027		f1: 0.7053		
epoch: 046		acc: 0.7271		nmi: 0.5266		ari: 0.5013		f1: 0.7046		
epoch: 047		acc: 0.7271		nmi: 0.5266		ari: 0.5013		f1: 0.7046		
epoch: 048		acc: 0.7275		nmi: 0.5266		ari: 0.5015		f1: 0.7052		
epoch: 049		acc: 0.7275		nmi: 0.5266		ari: 0.5015		f1: 0.7052		
epoch: 050		acc: 0.7275		nmi: 0.5266		ari: 0.5015		f1: 0.7052		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1094.90 MB.
Time consuming: 12.67s or 0.21m
====================Training loop No.7====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.4590		nmi: 0.2092		ari: 0.1535		f1: 0.2704		
epoch: 002		acc: 0.4225		nmi: 0.2053		ari: 0.0958		f1: 0.2122		
epoch: 003		acc: 0.4055		nmi: 0.2062		ari: 0.0665		f1: 0.1873		
epoch: 004		acc: 0.4372		nmi: 0.2517		ari: 0.1097		f1: 0.2215		
epoch: 005		acc: 0.4941		nmi: 0.3417		ari: 0.2012		f1: 0.4354		
epoch: 006		acc: 0.5923		nmi: 0.4115		ari: 0.2778		f1: 0.5467		
epoch: 007		acc: 0.5731		nmi: 0.4191		ari: 0.2966		f1: 0.4615		
epoch: 008		acc: 0.5982		nmi: 0.4300		ari: 0.3512		f1: 0.4734		
epoch: 009		acc: 0.6222		nmi: 0.4434		ari: 0.3889		f1: 0.4915		
epoch: 010		acc: 0.6414		nmi: 0.4482		ari: 0.4128		f1: 0.5205		
epoch: 011		acc: 0.7083		nmi: 0.4929		ari: 0.4800		f1: 0.6660		
epoch: 012		acc: 0.7430		nmi: 0.5255		ari: 0.5235		f1: 0.7200		
epoch: 013		acc: 0.7234		nmi: 0.5157		ari: 0.4784		f1: 0.7077		
epoch: 014		acc: 0.7035		nmi: 0.5046		ari: 0.4376		f1: 0.6947		
epoch: 015		acc: 0.7049		nmi: 0.5051		ari: 0.4389		f1: 0.6939		
epoch: 016		acc: 0.7127		nmi: 0.5030		ari: 0.4600		f1: 0.6905		
epoch: 017		acc: 0.7053		nmi: 0.5028		ari: 0.4682		f1: 0.6578		
epoch: 018		acc: 0.6787		nmi: 0.4900		ari: 0.4414		f1: 0.6065		
epoch: 019		acc: 0.6514		nmi: 0.4826		ari: 0.4276		f1: 0.5611		
epoch: 020		acc: 0.6555		nmi: 0.4861		ari: 0.4383		f1: 0.5577		
epoch: 021		acc: 0.6662		nmi: 0.4872		ari: 0.4499		f1: 0.5978		
epoch: 022		acc: 0.6839		nmi: 0.4972		ari: 0.4675		f1: 0.6309		
epoch: 023		acc: 0.7013		nmi: 0.5058		ari: 0.4820		f1: 0.6660		
epoch: 024		acc: 0.7131		nmi: 0.5153		ari: 0.4889		f1: 0.6875		
epoch: 025		acc: 0.7194		nmi: 0.5177		ari: 0.4909		f1: 0.6976		
epoch: 026		acc: 0.7245		nmi: 0.5204		ari: 0.4884		f1: 0.7065		
epoch: 027		acc: 0.7219		nmi: 0.5194		ari: 0.4791		f1: 0.7060		
epoch: 028		acc: 0.7238		nmi: 0.5229		ari: 0.4791		f1: 0.7085		
epoch: 029		acc: 0.7282		nmi: 0.5274		ari: 0.4879		f1: 0.7118		
epoch: 030		acc: 0.7304		nmi: 0.5281		ari: 0.4943		f1: 0.7125		
epoch: 031		acc: 0.7282		nmi: 0.5237		ari: 0.4947		f1: 0.7090		
epoch: 032		acc: 0.7278		nmi: 0.5241		ari: 0.4985		f1: 0.7066		
epoch: 033		acc: 0.7282		nmi: 0.5262		ari: 0.5047		f1: 0.7050		
epoch: 034		acc: 0.7242		nmi: 0.5249		ari: 0.5021		f1: 0.6994		
epoch: 035		acc: 0.7219		nmi: 0.5249		ari: 0.5015		f1: 0.6965		
epoch: 036		acc: 0.7208		nmi: 0.5258		ari: 0.5016		f1: 0.6948		
epoch: 037		acc: 0.7212		nmi: 0.5273		ari: 0.5018		f1: 0.6958		
epoch: 038		acc: 0.7212		nmi: 0.5278		ari: 0.5019		f1: 0.6959		
epoch: 039		acc: 0.7201		nmi: 0.5256		ari: 0.5008		f1: 0.6947		
epoch: 040		acc: 0.7216		nmi: 0.5258		ari: 0.5012		f1: 0.6968		
epoch: 041		acc: 0.7212		nmi: 0.5239		ari: 0.5003		f1: 0.6962		
epoch: 042		acc: 0.7201		nmi: 0.5230		ari: 0.4982		f1: 0.6953		
epoch: 043		acc: 0.7208		nmi: 0.5236		ari: 0.4988		f1: 0.6962		
epoch: 044		acc: 0.7208		nmi: 0.5234		ari: 0.4989		f1: 0.6961		
epoch: 045		acc: 0.7219		nmi: 0.5239		ari: 0.4994		f1: 0.6975		
epoch: 046		acc: 0.7227		nmi: 0.5242		ari: 0.4994		f1: 0.6986		
epoch: 047		acc: 0.7230		nmi: 0.5244		ari: 0.5001		f1: 0.6988		
epoch: 048		acc: 0.7234		nmi: 0.5246		ari: 0.5003		f1: 0.6993		
epoch: 049		acc: 0.7234		nmi: 0.5246		ari: 0.5003		f1: 0.6993		
epoch: 050		acc: 0.7234		nmi: 0.5246		ari: 0.5003		f1: 0.6993		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1094.90 MB.
Time consuming: 13.03s or 0.22m
====================Training loop No.8====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.2733		nmi: 0.1043		ari: -0.0105		f1: 0.1183		
epoch: 002		acc: 0.3191		nmi: 0.0433		ari: 0.0049		f1: 0.1082		
epoch: 003		acc: 0.4273		nmi: 0.2499		ari: 0.1076		f1: 0.2212		
epoch: 004		acc: 0.4295		nmi: 0.2540		ari: 0.0987		f1: 0.2632		
epoch: 005		acc: 0.5030		nmi: 0.3725		ari: 0.2129		f1: 0.4494		
epoch: 006		acc: 0.6145		nmi: 0.4650		ari: 0.2932		f1: 0.5714		
epoch: 007		acc: 0.5971		nmi: 0.4345		ari: 0.3370		f1: 0.4825		
epoch: 008		acc: 0.6193		nmi: 0.4382		ari: 0.3735		f1: 0.4909		
epoch: 009		acc: 0.6296		nmi: 0.4309		ari: 0.3815		f1: 0.5006		
epoch: 010		acc: 0.6388		nmi: 0.4372		ari: 0.3973		f1: 0.5110		
epoch: 011		acc: 0.6429		nmi: 0.4489		ari: 0.4077		f1: 0.5208		
epoch: 012		acc: 0.6784		nmi: 0.4749		ari: 0.4388		f1: 0.6238		
epoch: 013		acc: 0.7168		nmi: 0.5148		ari: 0.4909		f1: 0.6889		
epoch: 014		acc: 0.7079		nmi: 0.5090		ari: 0.4688		f1: 0.6889		
epoch: 015		acc: 0.6972		nmi: 0.4962		ari: 0.4473		f1: 0.6823		
epoch: 016		acc: 0.7024		nmi: 0.4982		ari: 0.4542		f1: 0.6868		
epoch: 017		acc: 0.7109		nmi: 0.5083		ari: 0.4747		f1: 0.6880		
epoch: 018		acc: 0.7072		nmi: 0.5102		ari: 0.4746		f1: 0.6713		
epoch: 019		acc: 0.6883		nmi: 0.4973		ari: 0.4593		f1: 0.6300		
epoch: 020		acc: 0.6706		nmi: 0.4928		ari: 0.4530		f1: 0.5957		
epoch: 021		acc: 0.6750		nmi: 0.4946		ari: 0.4635		f1: 0.6035		
epoch: 022		acc: 0.6817		nmi: 0.4980		ari: 0.4704		f1: 0.6200		
epoch: 023		acc: 0.6909		nmi: 0.4998		ari: 0.4744		f1: 0.6435		
epoch: 024		acc: 0.7016		nmi: 0.5089		ari: 0.4833		f1: 0.6657		
epoch: 025		acc: 0.7112		nmi: 0.5211		ari: 0.4958		f1: 0.6825		
epoch: 026		acc: 0.7164		nmi: 0.5288		ari: 0.4955		f1: 0.6932		
epoch: 027		acc: 0.7160		nmi: 0.5253		ari: 0.4871		f1: 0.6956		
epoch: 028		acc: 0.7164		nmi: 0.5228		ari: 0.4814		f1: 0.6977		
epoch: 029		acc: 0.7201		nmi: 0.5246		ari: 0.4844		f1: 0.7020		
epoch: 030		acc: 0.7253		nmi: 0.5285		ari: 0.4903		f1: 0.7076		
epoch: 031		acc: 0.7290		nmi: 0.5306		ari: 0.4979		f1: 0.7100		
epoch: 032		acc: 0.7286		nmi: 0.5288		ari: 0.5005		f1: 0.7078		
epoch: 033		acc: 0.7301		nmi: 0.5334		ari: 0.5076		f1: 0.7074		
epoch: 034		acc: 0.7297		nmi: 0.5327		ari: 0.5092		f1: 0.7058		
epoch: 035		acc: 0.7256		nmi: 0.5312		ari: 0.5074		f1: 0.7002		
epoch: 036		acc: 0.7238		nmi: 0.5309		ari: 0.5063		f1: 0.6980		
epoch: 037		acc: 0.7216		nmi: 0.5315		ari: 0.5054		f1: 0.6955		
epoch: 038		acc: 0.7212		nmi: 0.5312		ari: 0.5044		f1: 0.6953		
epoch: 039		acc: 0.7212		nmi: 0.5303		ari: 0.5035		f1: 0.6956		
epoch: 040		acc: 0.7201		nmi: 0.5291		ari: 0.5016		f1: 0.6946		
epoch: 041		acc: 0.7194		nmi: 0.5273		ari: 0.5013		f1: 0.6933		
epoch: 042		acc: 0.7186		nmi: 0.5269		ari: 0.5008		f1: 0.6925		
epoch: 043		acc: 0.7190		nmi: 0.5282		ari: 0.5013		f1: 0.6932		
epoch: 044		acc: 0.7201		nmi: 0.5294		ari: 0.5025		f1: 0.6943		
epoch: 045		acc: 0.7197		nmi: 0.5287		ari: 0.5023		f1: 0.6937		
epoch: 046		acc: 0.7205		nmi: 0.5290		ari: 0.5027		f1: 0.6946		
epoch: 047		acc: 0.7205		nmi: 0.5292		ari: 0.5028		f1: 0.6946		
epoch: 048		acc: 0.7205		nmi: 0.5292		ari: 0.5028		f1: 0.6946		
epoch: 049		acc: 0.7208		nmi: 0.5294		ari: 0.5030		f1: 0.6951		
epoch: 050		acc: 0.7205		nmi: 0.5288		ari: 0.5025		f1: 0.6947		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1094.90 MB.
Time consuming: 13.6s or 0.23m
====================Training loop No.9====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.4313		nmi: 0.1823		ari: 0.1423		f1: 0.2412		
epoch: 002		acc: 0.2869		nmi: 0.0606		ari: 0.0072		f1: 0.1179		
epoch: 003		acc: 0.4280		nmi: 0.2286		ari: 0.1033		f1: 0.1956		
epoch: 004		acc: 0.4180		nmi: 0.2261		ari: 0.0844		f1: 0.1945		
epoch: 005		acc: 0.5288		nmi: 0.3522		ari: 0.1859		f1: 0.3536		
epoch: 006		acc: 0.6263		nmi: 0.4730		ari: 0.3564		f1: 0.5895		
epoch: 007		acc: 0.5739		nmi: 0.4797		ari: 0.3283		f1: 0.4915		
epoch: 008		acc: 0.5779		nmi: 0.4821		ari: 0.3434		f1: 0.4766		
epoch: 009		acc: 0.6019		nmi: 0.4769		ari: 0.3698		f1: 0.4818		
epoch: 010		acc: 0.6071		nmi: 0.4534		ari: 0.3752		f1: 0.4814		
epoch: 011		acc: 0.6311		nmi: 0.4526		ari: 0.3978		f1: 0.5417		
epoch: 012		acc: 0.6769		nmi: 0.4757		ari: 0.4431		f1: 0.6230		
epoch: 013		acc: 0.7138		nmi: 0.5113		ari: 0.4869		f1: 0.6846		
epoch: 014		acc: 0.7360		nmi: 0.5305		ari: 0.5100		f1: 0.7177		
epoch: 015		acc: 0.7419		nmi: 0.5282		ari: 0.5131		f1: 0.7263		
epoch: 016		acc: 0.7400		nmi: 0.5264		ari: 0.5105		f1: 0.7222		
epoch: 017		acc: 0.7345		nmi: 0.5321		ari: 0.5056		f1: 0.7118		
epoch: 018		acc: 0.7105		nmi: 0.5239		ari: 0.4793		f1: 0.6636		
epoch: 019		acc: 0.6828		nmi: 0.5087		ari: 0.4516		f1: 0.6159		
epoch: 020		acc: 0.6658		nmi: 0.4942		ari: 0.4426		f1: 0.5901		
epoch: 021		acc: 0.6750		nmi: 0.4947		ari: 0.4553		f1: 0.6107		
epoch: 022		acc: 0.6828		nmi: 0.5024		ari: 0.4653		f1: 0.6279		
epoch: 023		acc: 0.6935		nmi: 0.5030		ari: 0.4705		f1: 0.6545		
epoch: 024		acc: 0.7053		nmi: 0.5141		ari: 0.4834		f1: 0.6755		
epoch: 025		acc: 0.7164		nmi: 0.5223		ari: 0.4937		f1: 0.6928		
epoch: 026		acc: 0.7312		nmi: 0.5344		ari: 0.5060		f1: 0.7124		
epoch: 027		acc: 0.7341		nmi: 0.5329		ari: 0.5055		f1: 0.7170		
epoch: 028		acc: 0.7352		nmi: 0.5335		ari: 0.5057		f1: 0.7177		
epoch: 029		acc: 0.7374		nmi: 0.5373		ari: 0.5084		f1: 0.7193		
epoch: 030		acc: 0.7367		nmi: 0.5368		ari: 0.5084		f1: 0.7174		
epoch: 031		acc: 0.7341		nmi: 0.5344		ari: 0.5072		f1: 0.7140		
epoch: 032		acc: 0.7301		nmi: 0.5315		ari: 0.5056		f1: 0.7075		
epoch: 033		acc: 0.7260		nmi: 0.5296		ari: 0.5030		f1: 0.7025		
epoch: 034		acc: 0.7230		nmi: 0.5302		ari: 0.5020		f1: 0.6983		
epoch: 035		acc: 0.7219		nmi: 0.5290		ari: 0.5011		f1: 0.6969		
epoch: 036		acc: 0.7212		nmi: 0.5295		ari: 0.5007		f1: 0.6962		
epoch: 037		acc: 0.7216		nmi: 0.5311		ari: 0.5022		f1: 0.6962		
epoch: 038		acc: 0.7216		nmi: 0.5313		ari: 0.5020		f1: 0.6967		
epoch: 039		acc: 0.7242		nmi: 0.5336		ari: 0.5045		f1: 0.7001		
epoch: 040		acc: 0.7249		nmi: 0.5320		ari: 0.5041		f1: 0.7012		
epoch: 041		acc: 0.7253		nmi: 0.5317		ari: 0.5045		f1: 0.7015		
epoch: 042		acc: 0.7264		nmi: 0.5313		ari: 0.5047		f1: 0.7029		
epoch: 043		acc: 0.7264		nmi: 0.5311		ari: 0.5045		f1: 0.7031		
epoch: 044		acc: 0.7264		nmi: 0.5302		ari: 0.5039		f1: 0.7033		
epoch: 045		acc: 0.7286		nmi: 0.5329		ari: 0.5074		f1: 0.7056		
epoch: 046		acc: 0.7286		nmi: 0.5329		ari: 0.5074		f1: 0.7056		
epoch: 047		acc: 0.7286		nmi: 0.5324		ari: 0.5078		f1: 0.7052		
epoch: 048		acc: 0.7286		nmi: 0.5324		ari: 0.5078		f1: 0.7052		
epoch: 049		acc: 0.7286		nmi: 0.5324		ari: 0.5078		f1: 0.7052		
epoch: 050		acc: 0.7286		nmi: 0.5324		ari: 0.5078		f1: 0.7052		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1094.90 MB.
Time consuming: 13.94s or 0.23m
====================Training loop No.10====================
GCSEE(
  (ae): AE(
    (enc_1): Linear(in_features=1433, out_features=500, bias=True)
    (enc_2): Linear(in_features=500, out_features=500, bias=True)
    (enc_3): Linear(in_features=500, out_features=2000, bias=True)
    (z_layer): Linear(in_features=2000, out_features=10, bias=True)
    (dec_1): Linear(in_features=10, out_features=2000, bias=True)
    (dec_2): Linear(in_features=2000, out_features=500, bias=True)
    (dec_3): Linear(in_features=500, out_features=500, bias=True)
    (x_bar_layer): Linear(in_features=500, out_features=1433, bias=True)
  )
  (gat): GAT(
    (conv1): GraphAttentionLayer (1433 -> 500)
    (conv2): GraphAttentionLayer (500 -> 500)
    (conv3): GraphAttentionLayer (500 -> 2000)
    (conv4): GraphAttentionLayer (2000 -> 10)
  )
  (gcn1): GCN(
    (activate): ReLU()
  )
  (gcn2): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn3): FAFGC(
    (mlp): Linear(in_features=1000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (gcn4): FAFGC(
    (mlp): Linear(in_features=4000, out_features=2, bias=True)
    (gcn): GCN(
      (activate): ReLU()
    )
  )
  (sf): SFGC(
    (mlp1): Linear(in_features=3020, out_features=5, bias=True)
    (scale_GCN): GCN()
  )
)
epoch: 001		acc: 0.4420		nmi: 0.2017		ari: 0.1345		f1: 0.2846		
epoch: 002		acc: 0.3338		nmi: 0.1622		ari: 0.1002		f1: 0.1542		
epoch: 003		acc: 0.3708		nmi: 0.1345		ari: 0.0265		f1: 0.1740		
epoch: 004		acc: 0.4417		nmi: 0.2508		ari: 0.1068		f1: 0.2731		
epoch: 005		acc: 0.5321		nmi: 0.4048		ari: 0.2793		f1: 0.5093		
epoch: 006		acc: 0.6477		nmi: 0.4920		ari: 0.3977		f1: 0.6264		
epoch: 007		acc: 0.6134		nmi: 0.4610		ari: 0.3795		f1: 0.4951		
epoch: 008		acc: 0.6156		nmi: 0.4430		ari: 0.3774		f1: 0.4823		
epoch: 009		acc: 0.6182		nmi: 0.4276		ari: 0.3726		f1: 0.4818		
epoch: 010		acc: 0.6259		nmi: 0.4233		ari: 0.3809		f1: 0.4937		
epoch: 011		acc: 0.6385		nmi: 0.4297		ari: 0.4033		f1: 0.5095		
epoch: 012		acc: 0.6725		nmi: 0.4496		ari: 0.4345		f1: 0.6029		
epoch: 013		acc: 0.7212		nmi: 0.4979		ari: 0.4866		f1: 0.6947		
epoch: 014		acc: 0.7175		nmi: 0.5022		ari: 0.4788		f1: 0.6949		
epoch: 015		acc: 0.7075		nmi: 0.5016		ari: 0.4564		f1: 0.6916		
epoch: 016		acc: 0.7024		nmi: 0.4965		ari: 0.4437		f1: 0.6891		
epoch: 017		acc: 0.7046		nmi: 0.5019		ari: 0.4494		f1: 0.6902		
epoch: 018		acc: 0.7031		nmi: 0.5001		ari: 0.4572		f1: 0.6793		
epoch: 019		acc: 0.6968		nmi: 0.4956		ari: 0.4565		f1: 0.6638		
epoch: 020		acc: 0.6854		nmi: 0.4849		ari: 0.4501		f1: 0.6416		
epoch: 021		acc: 0.6798		nmi: 0.4806		ari: 0.4526		f1: 0.6267		
epoch: 022		acc: 0.6765		nmi: 0.4754		ari: 0.4540		f1: 0.6199		
epoch: 023		acc: 0.6806		nmi: 0.4767		ari: 0.4551		f1: 0.6310		
epoch: 024		acc: 0.6950		nmi: 0.4909		ari: 0.4684		f1: 0.6574		
epoch: 025		acc: 0.7068		nmi: 0.5011		ari: 0.4787		f1: 0.6770		
epoch: 026		acc: 0.7142		nmi: 0.5144		ari: 0.4848		f1: 0.6895		
epoch: 027		acc: 0.7164		nmi: 0.5158		ari: 0.4831		f1: 0.6950		
epoch: 028		acc: 0.7168		nmi: 0.5147		ari: 0.4797		f1: 0.6969		
epoch: 029		acc: 0.7138		nmi: 0.5104		ari: 0.4734		f1: 0.6944		
epoch: 030		acc: 0.7160		nmi: 0.5132		ari: 0.4757		f1: 0.6972		
epoch: 031		acc: 0.7175		nmi: 0.5151		ari: 0.4779		f1: 0.6983		
epoch: 032		acc: 0.7186		nmi: 0.5146		ari: 0.4811		f1: 0.6985		
epoch: 033		acc: 0.7201		nmi: 0.5181		ari: 0.4861		f1: 0.6988		
epoch: 034		acc: 0.7227		nmi: 0.5193		ari: 0.4908		f1: 0.7012		
epoch: 035		acc: 0.7227		nmi: 0.5209		ari: 0.4939		f1: 0.7001		
epoch: 036		acc: 0.7260		nmi: 0.5248		ari: 0.5004		f1: 0.7032		
epoch: 037		acc: 0.7260		nmi: 0.5251		ari: 0.5010		f1: 0.7032		
epoch: 038		acc: 0.7242		nmi: 0.5251		ari: 0.5001		f1: 0.7005		
epoch: 039		acc: 0.7245		nmi: 0.5256		ari: 0.5010		f1: 0.7008		
epoch: 040		acc: 0.7242		nmi: 0.5257		ari: 0.5008		f1: 0.7006		
epoch: 041		acc: 0.7223		nmi: 0.5232		ari: 0.4980		f1: 0.6985		
epoch: 042		acc: 0.7223		nmi: 0.5233		ari: 0.4975		f1: 0.6988		
epoch: 043		acc: 0.7227		nmi: 0.5236		ari: 0.4982		f1: 0.6992		
epoch: 044		acc: 0.7238		nmi: 0.5250		ari: 0.5001		f1: 0.7002		
epoch: 045		acc: 0.7245		nmi: 0.5249		ari: 0.5015		f1: 0.7005		
epoch: 046		acc: 0.7242		nmi: 0.5247		ari: 0.5008		f1: 0.7002		
epoch: 047		acc: 0.7242		nmi: 0.5247		ari: 0.5008		f1: 0.7002		
epoch: 048		acc: 0.7242		nmi: 0.5247		ari: 0.5008		f1: 0.7002		
epoch: 049		acc: 0.7242		nmi: 0.5247		ari: 0.5008		f1: 0.7002		
epoch: 050		acc: 0.7242		nmi: 0.5247		ari: 0.5008		f1: 0.7002		
The total number of parameters is: 8.007854M(1e6).
The max memory allocated to model is: 1094.90 MB.
Time consuming: 14.3s or 0.24m
Namespace(is_pretrain=False, plot_clustering_tsne=False, plot_embedding_heatmap=False, adj_norm=False, adj_loop=True, adj_symmetric=True, desc='default', model_name='GCSEE', dataset_name='cora', root='/content/drive/MyDrive/GC-SEE', k=None, t=2, loops=10, feature_type='tensor', label_type='npy', adj_type='npy', seed=325, device='cuda', clusters=7, nodes=2708, lr=0.0006, max_epoch=50, pretrain_lr=0.001, pretrain_epoch=30, input_dim=1433, pretrain_ae_save_path='/content/drive/MyDrive/GC-SEE/pretrain/pretrain_ae/GCSEE/cora/', pretrain_gat_save_path='/content/drive/MyDrive/GC-SEE/pretrain/pretrain_gat/GCSEE/cora/', log_save_path='/content/drive/MyDrive/GC-SEE/logs/GCSEE/cora/', dataset_path='/content/drive/MyDrive/GC-SEE/', clustering_tsne_save_path='/content/drive/MyDrive/GC-SEE/img/clustering/GCSEE/cora/', embedding_heatmap_save_path='/content/drive/MyDrive/GC-SEE/img/heatmap/GCSEE/cora/', lambda3=0.001, lambda4=0.001, embedding_dim=10)
Total loops: 10
Mean value:
acc: 0.7360±0.0072		nmi: 0.5296±0.0039		ari: 0.5106±0.0083		f1: 0.7157±0.0081
Training over! Punch out!
